{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction\n",
    "- this is from the [sky-thought](https://github.com/NovaSky-AI/SkyThought/tree/main)\n",
    "- Maybe the repo is changing (like including best-of-N, control difficulty of dataset, et). if you wanna use, use the latest repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what I do\n",
    "    - create __init__.py to change the caceh dir of huggingface\n",
    "    - add result_dir in ./tools/eval.py\n",
    "    - change the system prompt as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the main function is math_eval/tools/inference_and_check.py: perform_inference_and_check\n",
    "    - each model have a system_prompt\n",
    "    - use TaskHandler to deal with each task to make conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_instruct_evals.md\teval.sh\t\t\t   README.md\n",
      "combine_data.py\t\tinference_and_check.py\t   requirements.txt\n",
      "convert_format.py\t__init__.py\t\t   response_rewrite.py\n",
      "convert_to_data.py\tlabeled_numina_difficulty  results\n",
      "eval.ipynb\t\tlabel_math_difficulty.py   upload_hub.py\n",
      "eval.py\t\t\t__pycache__\t\t   util\n",
      "base_instruct_evals.md\teval.sh\t\t\t   README.md\n",
      "combine_data.py\t\tinference_and_check.py\t   requirements.txt\n",
      "convert_format.py\t__init__.py\t\t   response_rewrite.py\n",
      "convert_to_data.py\tlabeled_numina_difficulty  results\n",
      "eval.ipynb\t\tlabel_math_difficulty.py   upload_hub.py\n",
      "eval.py\t\t\t__pycache__\t\t   util\n"
     ]
    }
   ],
   "source": [
    "!ls  # List files in the current directory (Linux/macOS)\n",
    "!dir  # List files in the current directory (Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eval AIME with command ['python', 'inference_and_check.py', '--model', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', '--dataset', 'AIME', '--split', 'train', '--tp', '1', '--result-dir', './results/generated/DeepSeek-R1-Distill-Qwen-7B', '--temperatures', '0.7']\n",
      "Temperature: [0.7]\n",
      "WARNING 02-17 21:01:08 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-17 21:01:08 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 02-17 21:01:08 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 02-17 21:01:22 model_runner.py:1014] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
      "INFO 02-17 21:01:26 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:15<00:15, 15.27s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:21<00:00, 10.14s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:21<00:00, 10.91s/it]\n",
      "\n",
      "INFO 02-17 21:01:48 model_runner.py:1025] Loading model weights took 14.2716 GB\n",
      "INFO 02-17 21:01:51 gpu_executor.py:122] # GPU blocks: 64893, # CPU blocks: 4681\n",
      "INFO 02-17 21:02:09 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-17 21:02:09 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-17 21:02:46 model_runner.py:1456] Graph capturing finished in 38 secs.\n",
      "Loaded 0 existing results.\n",
      "\n",
      "Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   3%|▎         | 1/30 [00:23<11:18, 23.41s/it, est. speed input: 14.69 toks/s, output: 61.13 toks/s]\n",
      "Processed prompts:   7%|▋         | 2/30 [00:29<06:16, 13.43s/it, est. speed input: 22.51 toks/s, output: 110.25 toks/s]\n",
      "Processed prompts:  10%|█         | 3/30 [00:40<05:28, 12.17s/it, est. speed input: 26.62 toks/s, output: 143.83 toks/s]\n",
      "Processed prompts:  13%|█▎        | 4/30 [00:45<03:57,  9.14s/it, est. speed input: 32.78 toks/s, output: 191.44 toks/s]\n",
      "Processed prompts:  17%|█▋        | 5/30 [00:47<02:47,  6.71s/it, est. speed input: 38.20 toks/s, output: 243.39 toks/s]\n",
      "Processed prompts:  20%|██        | 6/30 [00:51<02:22,  5.95s/it, est. speed input: 42.92 toks/s, output: 283.84 toks/s]\n",
      "Processed prompts:  23%|██▎       | 7/30 [00:59<02:25,  6.34s/it, est. speed input: 43.21 toks/s, output: 311.33 toks/s]\n",
      "Processed prompts:  27%|██▋       | 8/30 [01:02<01:56,  5.29s/it, est. speed input: 46.54 toks/s, output: 357.76 toks/s]\n",
      "Processed prompts:  30%|███       | 9/30 [01:17<02:57,  8.44s/it, est. speed input: 42.11 toks/s, output: 348.16 toks/s]\n",
      "Processed prompts:  33%|███▎      | 10/30 [01:27<03:00,  9.02s/it, est. speed input: 40.96 toks/s, output: 368.18 toks/s]\n",
      "Processed prompts:  37%|███▋      | 11/30 [01:41<03:16, 10.34s/it, est. speed input: 39.65 toks/s, output: 380.08 toks/s]\n",
      "Processed prompts:  40%|████      | 12/30 [01:56<03:34, 11.92s/it, est. speed input: 38.40 toks/s, output: 389.31 toks/s]\n",
      "Processed prompts:  43%|████▎     | 13/30 [02:22<04:33, 16.09s/it, est. speed input: 33.77 toks/s, output: 377.65 toks/s]\n",
      "Processed prompts:  47%|████▋     | 14/30 [02:32<03:48, 14.30s/it, est. speed input: 33.94 toks/s, output: 410.66 toks/s]\n",
      "Processed prompts:  50%|█████     | 15/30 [02:46<03:32, 14.15s/it, est. speed input: 33.23 toks/s, output: 434.19 toks/s]\n",
      "Processed prompts:  53%|█████▎    | 16/30 [02:53<02:50, 12.17s/it, est. speed input: 33.84 toks/s, output: 472.67 toks/s]\n",
      "Processed prompts:  57%|█████▋    | 17/30 [03:23<03:47, 17.53s/it, est. speed input: 30.70 toks/s, output: 459.66 toks/s]\n",
      "Processed prompts:  60%|██████    | 18/30 [03:29<02:47, 13.95s/it, est. speed input: 31.48 toks/s, output: 503.77 toks/s]\n",
      "Processed prompts:  63%|██████▎   | 19/30 [03:41<02:26, 13.28s/it, est. speed input: 31.56 toks/s, output: 533.31 toks/s]\n",
      "Processed prompts:  67%|██████▋   | 20/30 [04:06<02:48, 16.88s/it, est. speed input: 29.87 toks/s, output: 534.64 toks/s]\n",
      "Processed prompts:  70%|███████   | 21/30 [04:14<02:07, 14.20s/it, est. speed input: 30.59 toks/s, output: 573.76 toks/s]\n",
      "Processed prompts:  73%|███████▎  | 22/30 [04:16<01:24, 10.51s/it, est. speed input: 32.13 toks/s, output: 625.35 toks/s]\n",
      "Processed prompts:  77%|███████▋  | 23/30 [05:02<02:29, 21.29s/it, est. speed input: 28.61 toks/s, output: 585.36 toks/s]\n",
      "Processed prompts:  80%|████████  | 24/30 [05:28<02:16, 22.74s/it, est. speed input: 27.41 toks/s, output: 594.87 toks/s]\n",
      "Processed prompts:  83%|████████▎ | 25/30 [05:37<01:31, 18.37s/it, est. speed input: 27.70 toks/s, output: 636.51 toks/s]\n",
      "Processed prompts:  87%|████████▋ | 26/30 [05:38<00:53, 13.37s/it, est. speed input: 28.59 toks/s, output: 689.39 toks/s]\n",
      "Processed prompts:  90%|█████████ | 27/30 [06:43<01:26, 28.69s/it, est. speed input: 24.89 toks/s, output: 636.21 toks/s]\n",
      "Processed prompts:  93%|█████████▎| 28/30 [06:50<00:44, 22.28s/it, est. speed input: 26.08 toks/s, output: 681.95 toks/s]\n",
      "Processed prompts:  97%|█████████▋| 29/30 [07:23<00:25, 25.47s/it, est. speed input: 24.95 toks/s, output: 689.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [08:51<00:00, 44.21s/it, est. speed input: 21.51 toks/s, output: 635.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [08:51<00:00, 17.71s/it, est. speed input: 21.51 toks/s, output: 635.06 toks/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Processing Generations:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Processing Generations:  93%|█████████▎| 28/30 [00:02<00:00, 13.56it/s]\n",
      "Processing Generations: 100%|██████████| 30/30 [00:02<00:00, 14.30it/s]\n",
      "Final acc: 17/30\n",
      "{\"acc\": 0.5667}\n",
      "Token usage saved to ./results/generated/DeepSeek-R1-Distill-Qwen-7B/token_usage/DeepSeek-R1-Distill-Qwen-7B_AIME_train_None_0_-1.json\n",
      "Logs successfully written to ./results/eval/DeepSeek-R1-Distill-Qwen-7B.txt\n",
      "Results:\n",
      "{'AIME': 0.5667}\n"
     ]
    }
   ],
   "source": [
    "! python /home/wxy320/ondemand/program/llm_skills/math_eval/tools/eval.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --evals AIME \\\n",
    "    --tp 1 --output_file ./results/eval/DeepSeek-R1-Distill-Qwen-7B.txt \\\n",
    "    --result_dir ./results/generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
