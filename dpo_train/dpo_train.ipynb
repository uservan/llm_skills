{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/MSResearch_20200207_DeepZeroBlogGraphic_r2t3_1400x788-3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the deepspeed config: https://github.com/hiyouga/LLaMA-Factory/tree/main/examples/deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dpo Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use dpo_trainer\n",
    "    - refer to https://huggingface.co/docs/trl/v0.9.6/en/dpo_trainer\n",
    "    - two datasets format\n",
    "        - prompt, chosen, rejected\n",
    "        - conversation: system, user, assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "[2025-02-18 15:11:24,169] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wxy320/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/trl/import_utils.py\", line 180, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py\", line 47, in <module>\n",
      "    from .utils import (\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/trl/trainer/utils.py\", line 51, in <module>\n",
      "    import deepspeed\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/deepspeed/__init__.py\", line 25, in <module>\n",
      "    from . import ops\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/deepspeed/ops/__init__.py\", line 15, in <module>\n",
      "    from ..git_version_info import compatible_ops as __compatible_ops__\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/deepspeed/git_version_info.py\", line 29, in <module>\n",
      "    op_compatible = builder.is_compatible()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/deepspeed/ops/op_builder/fp_quantizer.py\", line 35, in is_compatible\n",
      "    sys_cuda_major, _ = installed_cuda_version()\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/deepspeed/ops/op_builder/builder.py\", line 53, in installed_cuda_version\n",
      "    output = subprocess.check_output([cuda_home + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/subprocess.py\", line 466, in check_output\n",
      "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/local/cuda/bin/nvcc'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wxy320/ondemand/program/llm_skills/dpo_train/dpo_train.py\", line 7, in <module>\n",
      "    from trl import DPOConfig, DPOTrainer\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/trl/import_utils.py\", line 171, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/trl/import_utils.py\", line 170, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/trl/import_utils.py\", line 182, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import trl.trainer.dpo_trainer because of the following error (look up to see its traceback):\n",
      "[Errno 13] Permission denied: '/usr/local/cuda/bin/nvcc'\n"
     ]
    }
   ],
   "source": [
    "! python dpo_train.py \\\n",
    "    --lr 3e-7 --beta 0.01 --model Instruct-3b \\\n",
    "    --dataset Bespoke_dpo --gradient_accumulation_steps 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use singularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! singularity build sglang.sif docker:lmsysorg/sglang:dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 15:17:59,955] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wxy320/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-02-18 15:18:04,375] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2025-02-18 15:18:04,376] [INFO] [runner.py:607:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None dpo_train.py --lr 3e-7 --beta 0.01 --model Instruct-3b --dataset Bespoke_dpo --gradient_accumulation_steps 4 --deepspeed /home/wxy320/ondemand/program/llm_skills/dpo_train/deepseed/zero3_config2.json\n",
      "[2025-02-18 15:18:06,066] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wxy320/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.21.5-1\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.21.5-1+cuda12.4\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.21.5-1\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.21.5-1+cuda12.4\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.21.5-1\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:164:main] dist_world_size=2\n",
      "[2025-02-18 15:18:08,583] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "[2025-02-18 15:18:08,584] [INFO] [launch.py:256:main] process 2058461 spawned with command: ['/usr/bin/python3', '-u', 'dpo_train.py', '--local_rank=0', '--lr', '3e-7', '--beta', '0.01', '--model', 'Instruct-3b', '--dataset', 'Bespoke_dpo', '--gradient_accumulation_steps', '4', '--deepspeed', '/home/wxy320/ondemand/program/llm_skills/dpo_train/deepseed/zero3_config2.json']\n",
      "[2025-02-18 15:18:08,585] [INFO] [launch.py:256:main] process 2058462 spawned with command: ['/usr/bin/python3', '-u', 'dpo_train.py', '--local_rank=1', '--lr', '3e-7', '--beta', '0.01', '--model', 'Instruct-3b', '--dataset', 'Bespoke_dpo', '--gradient_accumulation_steps', '4', '--deepspeed', '/home/wxy320/ondemand/program/llm_skills/dpo_train/deepseed/zero3_config2.json']\n",
      "[2025-02-18 15:18:13,372] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-18 15:18:13,377] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wxy320/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wxy320/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-02-18 15:18:16,444] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-02-18 15:18:16,444] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-02-18 15:18:16,444] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "README.md: 100%|██████████████████████████████| 24.0/24.0 [00:00<00:00, 299kB/s]\n",
      "Bespoke_dpo_filter.jsonl: 100%|██████████████| 162M/162M [00:03<00:00, 42.1MB/s]\n",
      "Generating train split: 100%|████| 10081/10081 [00:01<00:00, 8391.33 examples/s]\n",
      "config.json: 100%|█████████████████████████████| 661/661 [00:00<00:00, 8.40MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 35.6k/35.6k [00:00<00:00, 25.1MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/3.97G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/3.97G [00:00<01:35, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 21.0M/3.97G [00:00<01:33, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 31.5M/3.97G [00:00<01:34, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 41.9M/3.97G [00:00<01:33, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 52.4M/3.97G [00:01<01:33, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 62.9M/3.97G [00:01<01:32, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 73.4M/3.97G [00:01<01:32, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 83.9M/3.97G [00:01<01:32, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 94.4M/3.97G [00:02<01:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 105M/3.97G [00:02<01:32, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 115M/3.97G [00:02<01:31, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 126M/3.97G [00:02<01:30, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 136M/3.97G [00:03<01:31, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 147M/3.97G [00:03<01:30, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 157M/3.97G [00:03<01:29, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 168M/3.97G [00:03<01:29, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 178M/3.97G [00:04<01:29, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 189M/3.97G [00:04<01:29, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 199M/3.97G [00:04<01:28, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 210M/3.97G [00:04<01:27, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 220M/3.97G [00:05<01:27, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 231M/3.97G [00:05<01:27, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 241M/3.97G [00:05<01:27, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 252M/3.97G [00:05<01:27, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 262M/3.97G [00:06<01:27, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 273M/3.97G [00:06<01:27, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 283M/3.97G [00:06<01:26, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 294M/3.97G [00:06<01:27, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 304M/3.97G [00:07<01:26, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 315M/3.97G [00:07<01:25, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 325M/3.97G [00:07<01:25, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 336M/3.97G [00:07<01:25, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 346M/3.97G [00:08<01:24, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 357M/3.97G [00:08<01:24, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 367M/3.97G [00:08<01:24, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 377M/3.97G [00:08<01:23, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 388M/3.97G [00:09<01:23, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 398M/3.97G [00:09<01:23, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 409M/3.97G [00:09<01:23, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 419M/3.97G [00:09<01:23, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 430M/3.97G [00:10<01:22, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 440M/3.97G [00:10<01:22, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 451M/3.97G [00:10<01:11, 49.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 461M/3.97G [00:10<01:06, 52.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 472M/3.97G [00:10<01:11, 49.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 482M/3.97G [00:11<01:14, 46.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 493M/3.97G [00:11<01:16, 45.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 503M/3.97G [00:11<01:17, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 514M/3.97G [00:11<01:18, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 524M/3.97G [00:12<01:25, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 535M/3.97G [00:12<01:29, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 545M/3.97G [00:12<01:26, 39.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 556M/3.97G [00:13<01:24, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 566M/3.97G [00:13<01:22, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 577M/3.97G [00:13<01:21, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 587M/3.97G [00:13<01:20, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 598M/3.97G [00:14<01:20, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 608M/3.97G [00:14<01:19, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 619M/3.97G [00:14<01:19, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 629M/3.97G [00:14<01:19, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 640M/3.97G [00:14<01:18, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 650M/3.97G [00:15<01:17, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 661M/3.97G [00:15<01:17, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 671M/3.97G [00:15<01:16, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 682M/3.97G [00:15<01:16, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 692M/3.97G [00:16<01:16, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 703M/3.97G [00:16<01:16, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 713M/3.97G [00:16<01:16, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 724M/3.97G [00:16<01:16, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 734M/3.97G [00:17<01:15, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 744M/3.97G [00:17<01:17, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 755M/3.97G [00:17<01:14, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 765M/3.97G [00:17<01:14, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 776M/3.97G [00:18<01:13, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 786M/3.97G [00:18<01:13, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 797M/3.97G [00:18<01:13, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 807M/3.97G [00:18<01:13, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 818M/3.97G [00:19<01:13, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 828M/3.97G [00:19<01:13, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 839M/3.97G [00:19<01:13, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 849M/3.97G [00:19<01:12, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 860M/3.97G [00:20<01:12, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 870M/3.97G [00:20<01:12, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 881M/3.97G [00:20<01:12, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 891M/3.97G [00:20<01:12, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 902M/3.97G [00:21<01:12, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 912M/3.97G [00:21<01:12, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 923M/3.97G [00:21<01:11, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 933M/3.97G [00:21<01:11, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 944M/3.97G [00:22<01:10, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 954M/3.97G [00:22<01:10, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 965M/3.97G [00:22<01:09, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 975M/3.97G [00:22<01:09, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 986M/3.97G [00:23<01:09, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▎   | 996M/3.97G [00:23<01:09, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 1.01G/3.97G [00:23<01:09, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.02G/3.97G [00:23<01:08, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.03G/3.97G [00:24<01:08, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.04G/3.97G [00:24<01:08, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.05G/3.97G [00:24<01:08, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.06G/3.97G [00:24<01:07, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.07G/3.97G [00:25<01:07, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.08G/3.97G [00:25<01:07, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.09G/3.97G [00:25<01:07, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.10G/3.97G [00:25<01:06, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.11G/3.97G [00:26<01:06, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 1.12G/3.97G [00:26<01:06, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.13G/3.97G [00:26<01:05, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.14G/3.97G [00:26<01:05, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.15G/3.97G [00:26<01:05, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.16G/3.97G [00:27<01:05, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.17G/3.97G [00:27<01:05, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.18G/3.97G [00:27<01:05, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.20G/3.97G [00:27<01:04, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.21G/3.97G [00:28<01:04, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.22G/3.97G [00:28<01:04, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.23G/3.97G [00:28<01:03, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.24G/3.97G [00:28<01:03, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▎  | 1.25G/3.97G [00:29<01:03, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.26G/3.97G [00:29<01:03, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.27G/3.97G [00:29<01:03, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.28G/3.97G [00:29<01:02, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 1.29G/3.97G [00:30<01:02, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.30G/3.97G [00:30<01:02, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.31G/3.97G [00:30<01:09, 38.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 1.32G/3.97G [00:30<01:00, 43.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.33G/3.97G [00:31<01:00, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.34G/3.97G [00:31<01:00, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.35G/3.97G [00:31<01:00, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 1.36G/3.97G [00:31<01:00, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.37G/3.97G [00:32<01:00, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.38G/3.97G [00:32<01:00, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.39G/3.97G [00:32<01:00, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 1.41G/3.97G [00:32<00:59, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.42G/3.97G [00:33<00:59, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.43G/3.97G [00:33<00:59, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.44G/3.97G [00:33<00:59, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 1.45G/3.97G [00:33<00:58, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.46G/3.97G [00:34<00:58, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.47G/3.97G [00:34<00:58, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 1.48G/3.97G [00:34<00:58, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.49G/3.97G [00:34<00:57, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.50G/3.97G [00:35<00:58, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.51G/3.97G [00:35<00:57, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 1.52G/3.97G [00:35<00:57, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.53G/3.97G [00:35<00:56, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.54G/3.97G [00:36<00:56, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.55G/3.97G [00:36<00:56, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 1.56G/3.97G [00:36<00:56, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.57G/3.97G [00:36<00:56, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.58G/3.97G [00:37<00:55, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.59G/3.97G [00:37<00:56, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 1.60G/3.97G [00:37<00:56, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 1.61G/3.97G [00:37<00:56, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 1.63G/3.97G [00:38<00:55, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 1.64G/3.97G [00:38<00:55, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 1.65G/3.97G [00:38<00:56, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 1.66G/3.97G [00:38<00:55, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 1.67G/3.97G [00:39<00:54, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 1.68G/3.97G [00:39<00:54, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 1.69G/3.97G [00:39<00:53, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 1.70G/3.97G [00:39<00:53, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 1.71G/3.97G [00:40<00:53, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 1.72G/3.97G [00:40<00:53, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▋  | 1.73G/3.97G [00:40<00:53, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 1.74G/3.97G [00:40<00:52, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 1.75G/3.97G [00:41<00:52, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 1.76G/3.97G [00:41<00:53, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 1.77G/3.97G [00:41<00:53, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 1.78G/3.97G [00:41<00:51, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 1.79G/3.97G [00:42<00:51, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 1.80G/3.97G [00:42<00:51, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 1.81G/3.97G [00:42<00:51, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 1.82G/3.97G [00:42<00:51, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 1.84G/3.97G [00:43<00:50, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 1.85G/3.97G [00:43<00:50, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 1.86G/3.97G [00:43<00:50, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 1.87G/3.97G [00:43<00:49, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 1.88G/3.97G [00:44<00:49, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 1.89G/3.97G [00:44<00:50, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 1.90G/3.97G [00:44<00:49, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 1.91G/3.97G [00:44<00:48, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 1.92G/3.97G [00:45<00:48, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 1.93G/3.97G [00:45<00:49, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 1.94G/3.97G [00:45<00:55, 36.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 1.95G/3.97G [00:45<00:45, 44.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 1.96G/3.97G [00:46<00:46, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 1.97G/3.97G [00:46<00:47, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 1.98G/3.97G [00:46<00:46, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 1.99G/3.97G [00:46<00:46, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 2.00G/3.97G [00:47<00:46, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.01G/3.97G [00:47<00:48, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.02G/3.97G [00:47<00:47, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 2.03G/3.97G [00:47<00:45, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.04G/3.97G [00:48<00:45, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.06G/3.97G [00:48<00:45, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.07G/3.97G [00:48<00:45, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 2.08G/3.97G [00:48<00:44, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 2.09G/3.97G [00:49<00:43, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 2.10G/3.97G [00:49<00:45, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 2.11G/3.97G [00:49<00:45, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 2.12G/3.97G [00:49<00:44, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.13G/3.97G [00:50<00:43, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.14G/3.97G [00:50<00:43, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.15G/3.97G [00:50<00:43, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 2.16G/3.97G [00:50<00:42, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.17G/3.97G [00:51<00:42, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.18G/3.97G [00:51<00:43, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.19G/3.97G [00:51<00:42, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 2.20G/3.97G [00:51<00:41, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 2.21G/3.97G [00:52<00:41, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 2.22G/3.97G [00:52<00:42, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▎ | 2.23G/3.97G [00:52<00:41, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.24G/3.97G [00:52<00:40, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.25G/3.97G [00:53<00:40, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.26G/3.97G [00:53<00:41, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 2.28G/3.97G [00:53<00:40, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.29G/3.97G [00:53<00:40, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.30G/3.97G [00:54<00:39, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.31G/3.97G [00:54<00:39, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 2.32G/3.97G [00:54<00:39, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.33G/3.97G [00:54<00:38, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.34G/3.97G [00:55<00:38, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 2.35G/3.97G [00:55<00:38, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▍ | 2.36G/3.97G [00:55<00:38, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.37G/3.97G [00:55<00:43, 36.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.38G/3.97G [00:56<00:35, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 2.39G/3.97G [00:56<00:37, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 2.40G/3.97G [00:56<00:39, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 2.41G/3.97G [00:56<00:37, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 2.42G/3.97G [00:57<00:35, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 2.43G/3.97G [00:57<00:35, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 2.44G/3.97G [00:57<00:35, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 2.45G/3.97G [00:57<00:35, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 2.46G/3.97G [00:58<00:35, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 2.47G/3.97G [00:58<00:35, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 2.49G/3.97G [00:58<00:35, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 2.50G/3.97G [00:58<00:35, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 2.51G/3.97G [00:59<00:35, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 2.52G/3.97G [00:59<00:34, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 2.53G/3.97G [00:59<00:34, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 2.54G/3.97G [00:59<00:34, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 2.55G/3.97G [01:00<00:33, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 2.56G/3.97G [01:00<00:33, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 2.57G/3.97G [01:00<00:33, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 2.58G/3.97G [01:00<00:32, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 2.59G/3.97G [01:01<00:32, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▌ | 2.60G/3.97G [01:01<00:32, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 2.61G/3.97G [01:01<00:34, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 2.62G/3.97G [01:01<00:31, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 2.63G/3.97G [01:02<00:31, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 2.64G/3.97G [01:02<00:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 2.65G/3.97G [01:02<00:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 2.66G/3.97G [01:02<00:30, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 2.67G/3.97G [01:03<00:30, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 2.68G/3.97G [01:03<00:30, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 2.69G/3.97G [01:03<00:30, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 2.71G/3.97G [01:03<00:29, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 2.72G/3.97G [01:04<00:29, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▋ | 2.73G/3.97G [01:04<00:29, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 2.74G/3.97G [01:04<00:29, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 2.75G/3.97G [01:04<00:28, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 2.76G/3.97G [01:05<00:28, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 2.77G/3.97G [01:05<00:28, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 2.78G/3.97G [01:05<00:28, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 2.79G/3.97G [01:05<00:28, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 2.80G/3.97G [01:06<00:27, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 2.81G/3.97G [01:06<00:27, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 2.82G/3.97G [01:06<00:27, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 2.83G/3.97G [01:06<00:26, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 2.84G/3.97G [01:07<00:26, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 2.85G/3.97G [01:07<00:26, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 2.86G/3.97G [01:07<00:26, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 2.87G/3.97G [01:07<00:26, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 2.88G/3.97G [01:08<00:25, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 2.89G/3.97G [01:08<00:25, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 2.90G/3.97G [01:08<00:25, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 2.92G/3.97G [01:08<00:25, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 2.93G/3.97G [01:09<00:24, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 2.94G/3.97G [01:09<00:24, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 2.95G/3.97G [01:09<00:24, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 2.96G/3.97G [01:09<00:23, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 2.97G/3.97G [01:10<00:23, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 2.98G/3.97G [01:10<00:23, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 2.99G/3.97G [01:10<00:23, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.00G/3.97G [01:10<00:22, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.01G/3.97G [01:11<00:23, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.02G/3.97G [01:11<00:22, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 3.03G/3.97G [01:11<00:22, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.04G/3.97G [01:11<00:21, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.05G/3.97G [01:12<00:21, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.06G/3.97G [01:12<00:21, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 3.07G/3.97G [01:12<00:21, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 3.08G/3.97G [01:12<00:21, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 3.09G/3.97G [01:13<00:20, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 3.10G/3.97G [01:13<00:20, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 3.11G/3.97G [01:13<00:20, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.12G/3.97G [01:13<00:20, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.14G/3.97G [01:14<00:19, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 3.15G/3.97G [01:14<00:19, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.16G/3.97G [01:14<00:19, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.17G/3.97G [01:14<00:19, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.18G/3.97G [01:15<00:18, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 3.19G/3.97G [01:15<00:18, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 3.20G/3.97G [01:15<00:18, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 3.21G/3.97G [01:15<00:17, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 3.22G/3.97G [01:16<00:17, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▎| 3.23G/3.97G [01:16<00:17, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 3.24G/3.97G [01:16<00:17, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 3.25G/3.97G [01:16<00:16, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 3.26G/3.97G [01:17<00:19, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 3.27G/3.97G [01:17<00:15, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 3.28G/3.97G [01:17<00:15, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 3.29G/3.97G [01:17<00:15, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 3.30G/3.97G [01:18<00:15, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 3.31G/3.97G [01:18<00:15, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 3.32G/3.97G [01:18<00:15, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 3.33G/3.97G [01:18<00:14, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 3.34G/3.97G [01:19<00:14, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 3.36G/3.97G [01:19<00:14, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 3.37G/3.97G [01:19<00:14, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 3.38G/3.97G [01:19<00:14, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 3.39G/3.97G [01:20<00:13, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 3.40G/3.97G [01:20<00:13, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 3.41G/3.97G [01:20<00:13, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 3.42G/3.97G [01:20<00:13, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 3.43G/3.97G [01:21<00:12, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 3.44G/3.97G [01:21<00:12, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 3.45G/3.97G [01:21<00:12, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 3.46G/3.97G [01:21<00:12, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 3.47G/3.97G [01:22<00:11, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 3.48G/3.97G [01:22<00:11, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 3.49G/3.97G [01:22<00:11, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 3.50G/3.97G [01:22<00:11, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 3.51G/3.97G [01:23<00:10, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 3.52G/3.97G [01:23<00:10, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 3.53G/3.97G [01:23<00:10, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 3.54G/3.97G [01:23<00:10, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 3.55G/3.97G [01:24<00:09, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 3.57G/3.97G [01:24<00:09, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 3.58G/3.97G [01:24<00:09, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 3.59G/3.97G [01:24<00:09, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 3.60G/3.97G [01:25<00:08, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 3.61G/3.97G [01:25<00:08, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 3.62G/3.97G [01:25<00:08, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 3.63G/3.97G [01:25<00:08, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 3.64G/3.97G [01:26<00:07, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 3.65G/3.97G [01:26<00:07, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 3.66G/3.97G [01:26<00:07, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 3.67G/3.97G [01:26<00:07, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 3.68G/3.97G [01:27<00:06, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 3.69G/3.97G [01:27<00:06, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 3.70G/3.97G [01:27<00:07, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▋| 3.71G/3.97G [01:27<00:05, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 3.72G/3.97G [01:28<00:05, 44.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 3.73G/3.97G [01:28<00:05, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 3.74G/3.97G [01:28<00:05, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 3.75G/3.97G [01:28<00:05, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 3.76G/3.97G [01:29<00:04, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 3.77G/3.97G [01:29<00:04, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 3.79G/3.97G [01:29<00:04, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 3.80G/3.97G [01:29<00:04, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 3.81G/3.97G [01:30<00:03, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 3.82G/3.97G [01:30<00:03, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 3.83G/3.97G [01:30<00:03, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▊| 3.84G/3.97G [01:30<00:03, 39.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 3.85G/3.97G [01:31<00:02, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 3.86G/3.97G [01:31<00:02, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 3.87G/3.97G [01:31<00:02, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 3.88G/3.97G [01:31<00:02, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 3.89G/3.97G [01:32<00:01, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 3.90G/3.97G [01:32<00:01, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 3.91G/3.97G [01:32<00:01, 38.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 3.92G/3.97G [01:32<00:01, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 3.93G/3.97G [01:33<00:00, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 3.94G/3.97G [01:33<00:00, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 3.95G/3.97G [01:33<00:00, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 3.96G/3.97G [01:33<00:00, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|████| 3.97G/3.97G [01:33<00:00, 42.2MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 1/2 [01:34<01:34, 94.12s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/2.20G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|    | 10.5M/2.20G [00:00<00:52, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 21.0M/2.20G [00:00<00:51, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 31.5M/2.20G [00:00<00:50, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 41.9M/2.20G [00:00<00:50, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 52.4M/2.20G [00:01<00:50, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|    | 62.9M/2.20G [00:01<00:49, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏   | 73.4M/2.20G [00:01<00:49, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏   | 83.9M/2.20G [00:01<00:49, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏   | 94.4M/2.20G [00:02<00:49, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▏    | 105M/2.20G [00:02<00:49, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎    | 115M/2.20G [00:02<00:48, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 126M/2.20G [00:02<00:48, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 136M/2.20G [00:03<00:48, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 147M/2.20G [00:03<00:48, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 157M/2.20G [00:03<00:47, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 168M/2.20G [00:03<00:47, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 178M/2.20G [00:04<00:47, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 189M/2.20G [00:04<00:46, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 199M/2.20G [00:04<00:46, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▍    | 210M/2.20G [00:04<00:46, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▍    | 220M/2.20G [00:05<00:46, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌    | 231M/2.20G [00:05<00:46, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 241M/2.20G [00:05<00:46, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 252M/2.20G [00:05<00:45, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 262M/2.20G [00:06<00:45, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 273M/2.20G [00:06<00:45, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 283M/2.20G [00:06<00:45, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 294M/2.20G [00:06<00:44, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 304M/2.20G [00:07<00:44, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 315M/2.20G [00:07<00:45, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▋    | 325M/2.20G [00:07<00:43, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▊    | 336M/2.20G [00:07<00:43, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 346M/2.20G [00:08<00:43, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 357M/2.20G [00:08<00:43, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 367M/2.20G [00:08<00:43, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 377M/2.20G [00:08<00:43, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 388M/2.20G [00:09<00:42, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 398M/2.20G [00:09<00:42, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 409M/2.20G [00:09<00:42, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 419M/2.20G [00:09<00:41, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|▉    | 430M/2.20G [00:10<00:41, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|▉    | 440M/2.20G [00:10<00:41, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█    | 451M/2.20G [00:10<00:43, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 461M/2.20G [00:10<00:40, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 472M/2.20G [00:11<00:40, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 482M/2.20G [00:11<00:40, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 493M/2.20G [00:11<00:40, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 503M/2.20G [00:11<00:40, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 514M/2.20G [00:12<00:43, 39.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 524M/2.20G [00:12<00:38, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 535M/2.20G [00:12<00:38, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▏   | 545M/2.20G [00:12<00:38, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▎   | 556M/2.20G [00:13<00:38, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 566M/2.20G [00:13<00:38, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 577M/2.20G [00:13<00:38, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 587M/2.20G [00:13<00:38, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 598M/2.20G [00:14<00:37, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 608M/2.20G [00:14<00:37, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 619M/2.20G [00:14<00:37, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 629M/2.20G [00:14<00:37, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 640M/2.20G [00:15<00:36, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▍   | 650M/2.20G [00:15<00:36, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▍   | 661M/2.20G [00:15<00:36, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▌   | 671M/2.20G [00:15<00:36, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 682M/2.20G [00:16<00:35, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 692M/2.20G [00:16<00:35, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 703M/2.20G [00:16<00:34, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 713M/2.20G [00:16<00:34, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 724M/2.20G [00:17<00:34, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 734M/2.20G [00:17<00:34, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 744M/2.20G [00:17<00:34, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 755M/2.20G [00:17<00:34, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▋   | 765M/2.20G [00:18<00:33, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▊   | 776M/2.20G [00:18<00:36, 38.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 797M/2.20G [00:18<00:25, 54.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 807M/2.20G [00:18<00:26, 52.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 818M/2.20G [00:19<00:27, 49.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 828M/2.20G [00:19<00:29, 47.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 839M/2.20G [00:19<00:29, 45.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 849M/2.20G [00:19<00:32, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 860M/2.20G [00:19<00:29, 45.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▉   | 870M/2.20G [00:20<00:29, 44.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▉   | 881M/2.20G [00:20<00:30, 44.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|██   | 891M/2.20G [00:20<00:30, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 902M/2.20G [00:20<00:30, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 912M/2.20G [00:21<00:30, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|██   | 923M/2.20G [00:21<00:29, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|██   | 933M/2.20G [00:21<00:29, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 944M/2.20G [00:21<00:29, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 954M/2.20G [00:22<00:29, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|██▏  | 965M/2.20G [00:22<00:29, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|██▏  | 975M/2.20G [00:22<00:28, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▏  | 986M/2.20G [00:22<00:28, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▎  | 996M/2.20G [00:23<00:28, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.01G/2.20G [00:23<00:28, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.02G/2.20G [00:23<00:27, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▊  | 1.03G/2.20G [00:23<00:27, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▉  | 1.04G/2.20G [00:24<00:27, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.05G/2.20G [00:24<00:27, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.06G/2.20G [00:24<00:28, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.07G/2.20G [00:24<00:26, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.08G/2.20G [00:25<00:25, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.09G/2.20G [00:25<00:25, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|█▉  | 1.10G/2.20G [00:25<00:25, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██  | 1.11G/2.20G [00:25<00:25, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.12G/2.20G [00:26<00:25, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.13G/2.20G [00:26<00:24, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.14G/2.20G [00:26<00:24, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.15G/2.20G [00:26<00:24, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██  | 1.16G/2.20G [00:27<00:24, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▏ | 1.17G/2.20G [00:27<00:24, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.18G/2.20G [00:27<00:24, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.20G/2.20G [00:27<00:23, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.21G/2.20G [00:28<00:23, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.22G/2.20G [00:28<00:23, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▏ | 1.23G/2.20G [00:28<00:22, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▏ | 1.24G/2.20G [00:28<00:22, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.25G/2.20G [00:29<00:22, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.26G/2.20G [00:29<00:22, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 1.27G/2.20G [00:29<00:21, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 1.28G/2.20G [00:29<00:21, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 1.29G/2.20G [00:30<00:21, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 1.30G/2.20G [00:30<00:21, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▍ | 1.31G/2.20G [00:30<00:25, 35.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 1.32G/2.20G [00:30<00:21, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 1.33G/2.20G [00:31<00:19, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 1.34G/2.20G [00:31<00:19, 44.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 1.35G/2.20G [00:31<00:19, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 1.36G/2.20G [00:31<00:19, 43.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 1.37G/2.20G [00:32<00:19, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 1.38G/2.20G [00:32<00:19, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 1.39G/2.20G [00:32<00:19, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 1.41G/2.20G [00:32<00:19, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 1.42G/2.20G [00:33<00:18, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 1.43G/2.20G [00:33<00:17, 43.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 1.44G/2.20G [00:33<00:17, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 1.45G/2.20G [00:33<00:17, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 1.46G/2.20G [00:34<00:17, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 1.47G/2.20G [00:34<00:17, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 1.48G/2.20G [00:34<00:16, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 1.49G/2.20G [00:34<00:16, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 1.50G/2.20G [00:35<00:16, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▋ | 1.51G/2.20G [00:35<00:16, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 1.52G/2.20G [00:35<00:16, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 1.53G/2.20G [00:35<00:15, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 1.54G/2.20G [00:36<00:15, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 1.55G/2.20G [00:36<00:15, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|██▊ | 1.56G/2.20G [00:36<00:15, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|██▊ | 1.57G/2.20G [00:36<00:15, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▊ | 1.58G/2.20G [00:37<00:15, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▉ | 1.59G/2.20G [00:37<00:14, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 1.60G/2.20G [00:37<00:14, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 1.61G/2.20G [00:37<00:13, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|██▉ | 1.63G/2.20G [00:38<00:13, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|██▉ | 1.64G/2.20G [00:38<00:13, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|██▉ | 1.65G/2.20G [00:38<00:13, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███ | 1.66G/2.20G [00:38<00:12, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███ | 1.67G/2.20G [00:39<00:12, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███ | 1.68G/2.20G [00:39<00:12, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███ | 1.69G/2.20G [00:39<00:12, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███ | 1.70G/2.20G [00:39<00:12, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███ | 1.71G/2.20G [00:40<00:11, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███ | 1.72G/2.20G [00:40<00:11, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 1.73G/2.20G [00:40<00:11, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 1.74G/2.20G [00:40<00:10, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 1.75G/2.20G [00:41<00:10, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 1.76G/2.20G [00:41<00:10, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 1.77G/2.20G [00:41<00:10, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▏| 1.78G/2.20G [00:41<00:09, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▎| 1.79G/2.20G [00:42<00:09, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|███▎| 1.80G/2.20G [00:42<00:09, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|███▎| 1.81G/2.20G [00:42<00:09, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 1.82G/2.20G [00:42<00:09, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 1.84G/2.20G [00:43<00:08, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▎| 1.85G/2.20G [00:43<00:08, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▎| 1.86G/2.20G [00:43<00:08, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 1.87G/2.20G [00:43<00:07, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 1.88G/2.20G [00:44<00:07, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|███▍| 1.89G/2.20G [00:44<00:07, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|███▍| 1.90G/2.20G [00:44<00:07, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 1.91G/2.20G [00:44<00:06, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 1.92G/2.20G [00:45<00:06, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 1.93G/2.20G [00:45<00:06, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 1.94G/2.20G [00:45<00:06, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 1.95G/2.20G [00:45<00:06, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 1.96G/2.20G [00:46<00:05, 40.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 1.97G/2.20G [00:46<00:05, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 1.98G/2.20G [00:46<00:05, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 1.99G/2.20G [00:46<00:04, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▋| 2.00G/2.20G [00:47<00:04, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▋| 2.01G/2.20G [00:47<00:04, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|███▋| 2.02G/2.20G [00:47<00:04, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|███▋| 2.03G/2.20G [00:47<00:04, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 2.04G/2.20G [00:48<00:03, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 2.06G/2.20G [00:48<00:03, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|███▊| 2.07G/2.20G [00:48<00:03, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|███▊| 2.08G/2.20G [00:48<00:03, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|███▊| 2.09G/2.20G [00:49<00:02, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|███▊| 2.10G/2.20G [00:49<00:02, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 2.11G/2.20G [00:49<00:02, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 2.12G/2.20G [00:49<00:02, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|███▊| 2.13G/2.20G [00:50<00:01, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|███▉| 2.14G/2.20G [00:50<00:01, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 2.15G/2.20G [00:50<00:01, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 2.16G/2.20G [00:50<00:01, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 2.17G/2.20G [00:51<00:00, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 2.18G/2.20G [00:51<00:00, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 2.19G/2.20G [00:51<00:00, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|████| 2.20G/2.20G [00:51<00:00, 42.5MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [02:26<00:00, 73.04s/it]\n",
      "[2025-02-18 15:20:49,644] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [02:26<00:00, 73.03s/it]\n",
      "[2025-02-18 15:20:49,665] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-02-18 15:20:53,553] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 435, num_elems = 3.40B\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.51s/it]\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 3.25MB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.56s/it]\n",
      "tokenizer_config.json: 100%|███████████████| 7.30k/7.30k [00:00<00:00, 94.6MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 31.6MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 21.8MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 40.9MB/s]\n",
      "参数 'model.embed_tokens.weight' 可以更新梯度。\n",
      "参数 'model.embed_tokens.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.0.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.0.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.0.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.0.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.0.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.0.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.input_layernorm.weight' 可以更新梯度。参数 'model.layers.0.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.0.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.1.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.1.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.1.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.1.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.1.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.1.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.1.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.1.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.2.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.1.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.input_layernorm.weight' 可以更新梯度。参数 'model.layers.2.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.1.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.2.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.2.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.2.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.2.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.input_layernorm.weight' 可以更新梯度。参数 'model.layers.2.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.2.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.3.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.3.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.3.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.2.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.3.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.3.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.3.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.3.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.input_layernorm.weight' 可以更新梯度。参数 'model.layers.3.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.3.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.3.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.3.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.4.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.3.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.4.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.3.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.4.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.4.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.5.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.5.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.4.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.5.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.4.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.4.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.4.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.5.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.5.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.6.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.6.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.5.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.6.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.6.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.5.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.6.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.5.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.6.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.5.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.6.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.5.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.6.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.6.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.6.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.6.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.6.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.6.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.6.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.6.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.7.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.8.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.8.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.7.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.8.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.7.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.8.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.7.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.8.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.8.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.8.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.input_layernorm.weight' 可以更新梯度。参数 'model.layers.8.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.7.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.8.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.9.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.8.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.9.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.8.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.9.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.9.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.8.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.8.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.9.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.9.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.9.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.9.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.9.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.9.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.9.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.10.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.10.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.10.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.10.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.10.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.11.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.11.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.10.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.10.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.10.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.11.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.11.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.11.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.11.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.12.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.12.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.11.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.13.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.13.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.11.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.13.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.input_layernorm.weight' 可以更新梯度。参数 'model.layers.13.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.11.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.13.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.13.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.13.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.12.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.14.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.12.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.12.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.12.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.13.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.13.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.input_layernorm.weight' 可以更新梯度。参数 'model.layers.13.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.15.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.13.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.13.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.13.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.15.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.input_layernorm.weight' 可以更新梯度。参数 'model.layers.14.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.14.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.16.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.14.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.16.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.16.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.14.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.16.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.16.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.15.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.15.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.15.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.15.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.input_layernorm.weight' 可以更新梯度。参数 'model.layers.17.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.15.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.16.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.16.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.18.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.16.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.16.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.16.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.16.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.18.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.16.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.18.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.16.input_layernorm.weight' 可以更新梯度。参数 'model.layers.18.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.16.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.18.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.19.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.19.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.19.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.17.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.19.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.17.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.17.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.19.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.17.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.input_layernorm.weight' 可以更新梯度。参数 'model.layers.17.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.20.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.20.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.20.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.20.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.20.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.20.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.20.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.20.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.18.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.21.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.19.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.19.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.19.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.19.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.19.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.21.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.19.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.21.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.19.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.22.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.input_layernorm.weight' 可以更新梯度。参数 'model.layers.22.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.19.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.20.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.20.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.22.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.20.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.22.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.20.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.20.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.input_layernorm.weight' 可以更新梯度。参数 'model.layers.20.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.23.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.23.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.21.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.23.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.21.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.23.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.23.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.23.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.21.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.24.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.21.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.21.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.21.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.24.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.22.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.22.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.24.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.24.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.22.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.25.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.22.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.input_layernorm.weight' 可以更新梯度。参数 'model.layers.25.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.22.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.25.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.23.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.input_layernorm.weight' 可以更新梯度。参数 'model.layers.23.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.23.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.23.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.26.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.23.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.26.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.23.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.24.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.26.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.24.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.27.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.24.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.24.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.24.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.27.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.27.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.24.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.27.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.24.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.27.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.28.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.28.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.28.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.28.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.25.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.input_layernorm.weight' 可以更新梯度。参数 'model.layers.28.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.25.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.26.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.29.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.26.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.26.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.26.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.26.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.29.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.29.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.29.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.26.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.30.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.30.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.30.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.27.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.27.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.27.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.27.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.30.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.27.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.input_layernorm.weight' 可以更新梯度。参数 'model.layers.27.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.27.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.input_layernorm.weight' 可以更新梯度。参数 'model.layers.31.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.27.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.31.self_attn.v_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.31.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.28.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.28.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.28.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.31.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.28.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.32.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.28.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.32.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.28.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.input_layernorm.weight' 可以更新梯度。参数 'model.layers.32.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.28.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.32.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.29.self_attn.k_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.input_layernorm.weight' 可以更新梯度。参数 'model.layers.29.self_attn.v_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.self_attn.v_proj.bias' 可以更新梯度。参数 'model.layers.32.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.gate_proj.weight' 可以更新梯度。参数 'model.layers.33.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.29.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.33.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.33.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.29.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.33.self_attn.v_proj.weight' 可以更新梯度。参数 'model.layers.29.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.33.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.q_proj.weight' 可以更新梯度。参数 'model.layers.33.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.self_attn.q_proj.bias' 可以更新梯度。参数 'model.layers.33.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.33.mlp.down_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.30.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.34.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.34.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.30.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.34.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.up_proj.weight' 可以更新梯度。参数 'model.layers.31.self_attn.q_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.34.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.31.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.self_attn.k_proj.weight' 可以更新梯度。参数 'model.layers.34.input_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.self_attn.k_proj.bias' 可以更新梯度。参数 'model.layers.34.post_attention_layernorm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.o_proj.weight' 可以更新梯度。参数 'model.layers.35.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.35.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.down_proj.weight' 可以更新梯度。参数 'model.layers.35.self_attn.o_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.input_layernorm.weight' 可以更新梯度。参数 'model.layers.35.mlp.gate_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.31.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.35.mlp.up_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.35.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.input_layernorm.weight' 可以更新梯度。参数 'model.layers.32.self_attn.q_proj.bias' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.35.post_attention_layernorm.weight' 可以更新梯度。参数 'model.layers.32.self_attn.k_proj.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.self_attn.k_proj.bias' 可以更新梯度。参数 'model.norm.weight' 可以更新梯度。\n",
      "\n",
      "参数 'model.layers.32.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.norm.weight' 可以更新梯度。\n",
      "LLM总参数量：0.000 百万\n",
      "LLM总参数量：0.000 百万\n",
      "[2025-02-18 15:21:00,144] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-02-18 15:21:00,149] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-02-18 15:21:00,402] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 870, num_elems = 6.79B\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.49s/it]\n",
      "参数 'model.embed_tokens.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.norm.weight' 可以更新梯度。\n",
      "LLM总参数量：0.000 百万\n",
      "参数 'model.embed_tokens.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.0.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.0.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.1.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.1.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.2.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.2.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.3.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.3.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.4.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.4.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.5.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.5.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.6.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.6.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.7.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.7.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.8.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.8.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.9.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.9.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.10.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.10.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.11.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.11.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.12.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.12.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.13.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.13.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.14.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.14.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.15.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.15.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.16.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.16.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.17.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.17.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.18.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.18.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.19.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.19.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.20.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.20.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.21.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.21.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.22.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.22.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.23.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.23.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.24.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.24.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.25.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.25.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.26.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.26.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.27.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.27.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.28.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.28.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.29.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.29.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.30.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.30.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.31.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.31.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.32.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.32.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.33.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.33.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.34.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.34.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.q_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.k_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.k_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.v_proj.bias' 可以更新梯度。\n",
      "参数 'model.layers.35.self_attn.o_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.gate_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.up_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.mlp.down_proj.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.input_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.layers.35.post_attention_layernorm.weight' 可以更新梯度。\n",
      "参数 'model.norm.weight' 可以更新梯度。\n",
      "LLM总参数量：0.000 百万\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
      "Extracting prompt in train dataset: 100%|█| 10081/10081 [00:03<00:00, 3029.08 ex\n",
      "Applying chat template to train dataset: 100%|█| 10081/10081 [00:04<00:00, 2396.\n",
      "Tokenizing train dataset: 100%|███| 10081/10081 [01:28<00:00, 114.04 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-02-18 15:22:43,896] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-02-18 15:22:43,896] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-02-18 15:22:43,907] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-02-18 15:22:43,909] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2025-02-18 15:22:44,085] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-02-18 15:22:44,152] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-02-18 15:22:44,152] [INFO] [utils.py:782:see_memory_usage] MA 5.81 GB         Max_MA 6.97 GB         CA 7.39 GB         Max_CA 7 GB \n",
      "[2025-02-18 15:22:44,153] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.27 GB, percent = 1.6%\n",
      "Parameter Offload: Total persistent parameters: 241664 in 181 params\n",
      "[2025-02-18 15:22:44,366] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-02-18 15:22:44,367] [INFO] [utils.py:782:see_memory_usage] MA 5.81 GB         Max_MA 5.81 GB         CA 7.39 GB         Max_CA 7 GB \n",
      "[2025-02-18 15:22:44,367] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.27 GB, percent = 1.6%\n",
      "[2025-02-18 15:22:44,368] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-02-18 15:22:44,368] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-02-18 15:22:44,368] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-02-18 15:22:44,368] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-02-18 15:22:44,368] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1c70514be0>\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-02-18 15:22:44,369] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2025-02-18 15:22:44,370] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   loss_scale ................... 1.0\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-02-18 15:22:44,371] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   steps_per_print .............. inf\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   train_batch_size ............. 8\n",
      "[2025-02-18 15:22:44,372] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2025-02-18 15:22:44,373] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": \"auto\", \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization.reduce_bucket_size\": 4.194304e+06, \n",
      "    \"zero_optimization.stage3_param_persistence_threshold\": 2.048000e+04, \n",
      "    \"zero_optimization.stage3_prefetch_bucket_size\": 3.774874e+06\n",
      "}\n",
      "Parameter Offload: Total persistent parameters: 241664 in 181 params\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 0.6914, 'grad_norm': 27.232018170053443, 'learning_rate': 2.3809523809523806e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -2960.0, 'logps/rejected': -73.875, 'logits/chosen': -0.9541015625, 'logits/rejected': -3.03125, 'epoch': 0.0}\n",
      "{'loss': 0.6914, 'grad_norm': 17.766057307068806, 'learning_rate': 4.761904761904761e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -1648.0, 'logps/rejected': -85.25, 'logits/chosen': -1.212890625, 'logits/rejected': -3.04296875, 'epoch': 0.0}\n",
      "{'loss': 0.6914, 'grad_norm': 16.125698749663282, 'learning_rate': 7.142857142857142e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.00031280517578125, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00031280517578125, 'logps/chosen': -1616.0, 'logps/rejected': -94.625, 'logits/chosen': -1.2861328125, 'logits/rejected': -3.23828125, 'epoch': 0.0}\n",
      "{'loss': 0.7021, 'grad_norm': 22.88950335631744, 'learning_rate': 9.523809523809522e-09, 'rewards/chosen': -0.02001953125, 'rewards/rejected': -0.00156402587890625, 'rewards/accuracies': 0.375, 'rewards/margins': -0.01845550537109375, 'logps/chosen': -2748.0, 'logps/rejected': -644.75, 'logits/chosen': -1.166015625, 'logits/rejected': nan, 'epoch': 0.0}\n",
      "{'loss': 0.6982, 'grad_norm': 20.87789445232387, 'learning_rate': 1.1904761904761903e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.00156402587890625, 'rewards/accuracies': 0.375, 'rewards/margins': -0.0015411376953125, 'logps/chosen': -2110.0, 'logps/rejected': -77.9375, 'logits/chosen': -1.109375, 'logits/rejected': -3.2265625, 'epoch': 0.0}\n",
      "{'loss': 0.6865, 'grad_norm': 19.281498847878602, 'learning_rate': 1.4285714285714284e-08, 'rewards/chosen': 0.010009765625, 'rewards/rejected': -0.0018768310546875, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0118560791015625, 'logps/chosen': -2276.0, 'logps/rejected': -370.6875, 'logits/chosen': -1.05078125, 'logits/rejected': nan, 'epoch': 0.0}\n",
      "{'loss': 0.6841, 'grad_norm': 18.062290637149534, 'learning_rate': 1.6666666666666664e-08, 'rewards/chosen': 0.010009765625, 'rewards/rejected': -0.00250244140625, 'rewards/accuracies': 0.625, 'rewards/margins': 0.01251220703125, 'logps/chosen': -1728.0, 'logps/rejected': -81.4375, 'logits/chosen': -1.140625, 'logits/rejected': -3.078125, 'epoch': 0.01}\n",
      "{'loss': 0.6968, 'grad_norm': 19.492215585059874, 'learning_rate': 1.9047619047619045e-08, 'rewards/chosen': -0.010009765625, 'rewards/rejected': -0.002349853515625, 'rewards/accuracies': 0.375, 'rewards/margins': -0.0076751708984375, 'logps/chosen': -1912.0, 'logps/rejected': -57.5625, 'logits/chosen': -1.32421875, 'logits/rejected': -3.375, 'epoch': 0.01}\n",
      "{'loss': 0.6865, 'grad_norm': 19.393439734770105, 'learning_rate': 2.1428571428571426e-08, 'rewards/chosen': 0.010009765625, 'rewards/rejected': -0.00218963623046875, 'rewards/accuracies': 0.625, 'rewards/margins': 0.012176513671875, 'logps/chosen': -1830.0, 'logps/rejected': -90.0625, 'logits/chosen': -1.14453125, 'logits/rejected': -3.03515625, 'epoch': 0.01}\n",
      "  1%|▎                                       | 9/1260 [01:28<3:01:59,  8.73s/it][2025-02-18 15:24:27,207] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2058461\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "[2025-02-18 15:24:27,259] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2058461\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wxy320/.local/bin/deepspeed\", line 6, in <module>\n",
      "    main()\n",
      "  File \"/home/wxy320/.local/lib/python3.10/site-packages/deepspeed/launcher/runner.py\", line 623, in main\n",
      "    result.wait()\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! singularity exec --nv --bind $HOME:$HOME --bind /tmp:/tmp --bind /scratch:/scratch \\\n",
    "    --env PATH=\"$HOME/.local/bin:$PATH\" $HOME/sglang.sif \\\n",
    "    deepspeed dpo_train.py \\\n",
    "    --lr 3e-7 --beta 0.01 --model Instruct-3b --dataset Bespoke_dpo --gradient_accumulation_steps 4 \\\n",
    "    --deepspeed /home/wxy320/ondemand/program/llm_skills/dpo_train/deepseed/zero3_config2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/hiyouga/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multio1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
